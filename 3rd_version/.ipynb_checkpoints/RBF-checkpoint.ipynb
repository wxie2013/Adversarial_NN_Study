{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is to check adversarial attached on different neural networks. The following tutorials or codes have been very useful during the development: \n",
    "    \n",
    "    https://github.com/PetraVidnerova/rbf_keras\n",
    "    https://keras.io/layers/writing-your-own-keras-layers/\n",
    "    https://nextjournal.com/gkoehler/digit-recognition-with-keras\n",
    "    https://fairyonice.github.io/Generate-adversarial-examples-using-TensorFlow.html\n",
    "    https://www.anishathalye.com/2017/07/25/synthesizing-adversarial-examples/\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "import gzip\n",
    "import _pickle as cPickle\n",
    "\n",
    "\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.initializers import RandomUniform, Initializer, Orthogonal, Constant\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from tensorflow.python.framework import graph_util\n",
    "from tensorflow.python.framework import graph_io\n",
    "\n",
    "import math\n",
    "import random \n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_____initializer for RBF\n",
    "class InitCentersKmean(Initializer):\n",
    "    \"\"\" Initializer for initialization of centers of RBF network\n",
    "        using K-mean.\n",
    "\n",
    "    # Arguments\n",
    "        X: matrix, dataset to choose the centers from (random rows \n",
    "          are taken as centers)\n",
    "    \"\"\"\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "        print(\"_____ initializing center using InitCentersKmean _____\")\n",
    "\n",
    "    def __call__(self, shape, dtype=None):\n",
    "        # \"shape\" is the input_shape for \"self.add_weigh\" down below\n",
    "        self.center = sklearn.cluster.k_means(self.X, init='random', n_clusters=shape[0], max_iter = 1, n_jobs=-1)\n",
    "        \n",
    "        return self.center[0]\n",
    "\n",
    "#______ the custom RBF layer        \n",
    "class RBFLayer(Layer):\n",
    "    \"\"\" Layer of Gaussian RBF units. \n",
    "\n",
    "    # Example\n",
    " \n",
    "    ```python\n",
    "        model = Sequential()\n",
    "        model.add(RBFLayer(10,\n",
    "                           initializer=InitCentersRandom(X), \n",
    "                           betas=1.0,\n",
    "                           input_shape=(1,)))\n",
    "        model.add(Dense(1))\n",
    "    ```\n",
    "    \n",
    "\n",
    "    # Arguments\n",
    "        output_dim: number of hidden units (i.e. number of outputs of the layer)\n",
    "        initializer: instance of initiliazer to initialize centers\n",
    "        betas: float, initial value for betas \n",
    "\n",
    "    \"\"\"\n",
    "   \n",
    "    def __init__(self, output_dim, betas, initializer, **kwargs):\n",
    "        self.output_dim = output_dim # dim of the layer's output, i.e. in this case: number of centers in this layer\n",
    "        self.init_betas = betas \n",
    "        self.initializer = initializer \n",
    "\n",
    "        super().__init__(**kwargs) #python3 format\n",
    "\n",
    "    # input shape is the shape from output of last layers. in this case, it's data: [?, 784]\n",
    "    def build(self, input_shape): \n",
    "        self.centers = self.add_weight(name='centers', \n",
    "                                       shape=(self.output_dim, input_shape[1]),\n",
    "                                       initializer=self.initializer,\n",
    "                                       trainable=True)\n",
    "        \n",
    "        self.betas = self.add_weight(name='betas',\n",
    "                                     shape=(self.output_dim,), \n",
    "                                     initializer=Constant(value=self.init_betas),\n",
    "                                     trainable=True)\n",
    "        \n",
    "        super().build(input_shape) #python3\n",
    "     \n",
    "    def call(self, x):\n",
    "\n",
    "        C=K.expand_dims(self.centers) # shape (ncenter, 784, 1) for broadcast\n",
    "        H = K.transpose(C-K.transpose(x)) # K.transpose(x) shape (784, ?), then this shape: \n",
    "\n",
    "        \"\"\"\n",
    "        # the following printout is used for debugging purpose\n",
    "        print(\"self.centers.shape: \", self.centers.shape)\n",
    "        print(\"x.shape\", x.shape)\n",
    "        print(\"K.transpose(x).shape: \", K.transpose(x).shape)\n",
    "        print(\"C.shape: \", C.shape)\n",
    "        print(\"H.shape: \", H.shape)\n",
    "        print(\"self.betas.shape: \", self.betas.shape)\n",
    "        print(\"K.sum(H**2, axis=-2).shape: \", K.sum(H**2, axis=-2).shape)\n",
    "        print(\"call return shape: \", K.exp( -self.betas * K.sum(H**2, axis=-2)).shape)\n",
    "        \"\"\"\n",
    "        a = K.exp( -K.sum(H**2, axis=-2)/(2*self.betas*self.betas))\n",
    "        \n",
    "        return a\n",
    "             \n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        # have to define get_config to be able to use model_from_json\n",
    "        config = {\n",
    "            'output_dim': self.output_dim\n",
    "        }\n",
    "        #base_config = super(RBFLayer, self).get_config()\n",
    "        base_config = super().get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#___ load and normalize mnist data \n",
    "def load_mnist_data(n_classes):\n",
    "\n",
    "    (X_train_raw, Y_train_raw), (X_test_raw, Y_test_raw) = mnist.load_data()\n",
    "    \n",
    "    #plt.imshow(X_train_raw[0])\n",
    "    # Normalizing the input data to max=1 helps to speed up the training. \n",
    "    # reshape each image to 1D vector for simplicity\n",
    "    n_train = X_train_raw.shape[0]\n",
    "    n_test = X_test_raw.shape[0]\n",
    "    v_len = X_train_raw.shape[1]*X_train_raw.shape[2]    \n",
    "    \n",
    "    X_train = X_train_raw.reshape(n_train, v_len)\n",
    "    X_test = X_test_raw.reshape(n_test, v_len)\n",
    "    \n",
    "    # normalizing the data to help with the training. accurancy is lower than sklearn.preprocessing.scale\n",
    "    \"\"\"\n",
    "    max_pixel = np.max(X_train)\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    X_train /= max_pixel\n",
    "    X_test /= max_pixel\n",
    "    \"\"\"\n",
    "    #using sklearn.preprocessing.scale, leads to higher accuracy than normalization by max as above\n",
    "    X_train = scale(X_train)\n",
    "    X_test = scale(X_test)\n",
    "\n",
    "    \n",
    "    #Do it only when the final layers output is, e.g. 10\n",
    "    # one-hot encoding using keras' numpy-related utilities\n",
    "    Y_train = np_utils.to_categorical(Y_train_raw, n_classes)\n",
    "    Y_test = np_utils.to_categorical(Y_test_raw, n_classes)\n",
    "    \n",
    "    return X_train_raw, Y_train_raw, X_test_raw, Y_test_raw, X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__ Neural network model\n",
    "class NN_model():\n",
    "    def __init__(self, name, X, n_classes, n_center, beta, initializer):\n",
    "                \n",
    "        self.model = Sequential()\n",
    "                    \n",
    "        if  \"RBF\" in name: # need to change number of centers in the RBF\n",
    "            print(\"++++ using RBF network +++++\")\n",
    "            one_sample_shape = X.shape[-1] # shape for each individual sample\n",
    "\n",
    "            self.rbflayer = RBFLayer(n_center, beta, \n",
    "                                    initializer=initializer(X),\n",
    "                                    # input_shape is the data shape. The 2nd dim of (one_sample_shape, ) is the number of samples and is treated as ? in keras\n",
    "                                    input_shape=(one_sample_shape, ), name=name) \n",
    "\n",
    "            self.model.add(self.rbflayer)\n",
    "            self.model.add(Dense(n_classes, name='logits'))\n",
    "            self.model.add(Activation('softmax'))\n",
    "        elif name==\"dense_1L\": \n",
    "            print(\"++++ using 1-hidden layer w/10 neutons dense network +++++\")\n",
    "            self.model.add(Dense(10, input_shape=(784,), name=name))\n",
    "            self.model.add(Activation('relu'))                            \n",
    "            #self.model.add(Dropout(0.2))\n",
    "            self.model.add(Dense(n_classes, name='logits'))\n",
    "            self.model.add(Activation('softmax'))\n",
    "        elif name==\"dense\": \n",
    "            print(\"++++ using 2-hidden layer w/512 neurons dense network +++++\")\n",
    "            self.model.add(Dense(512, input_shape=(784,), name=name))\n",
    "            self.model.add(Activation('relu'))                            \n",
    "            #self.model.add(Dropout(0.2))\n",
    "\n",
    "            self.model.add(Dense(512))\n",
    "            self.model.add(Activation('relu'))\n",
    "            #self.model.add(Dropout(0.2))\n",
    "\n",
    "            self.model.add(Dense(n_classes, name='logits'))\n",
    "            self.model.add(Activation('softmax'))\n",
    "        else:\n",
    "            print(\"+++++ only option: RBF or dense_1L or dense ++++\")\n",
    "            exit()\n",
    "                             # file and directory for storing weight\n",
    "        self.weight_dir = 'keras_model_parameter/'\n",
    "        self.weight_file = self.weight_dir+name+'_weights.h5'\n",
    "        if not os.path.isdir(self.weight_dir):\n",
    "            os.mkdir(self.weight_dir)\n",
    "    \n",
    "        self.model.summary()\n",
    "        self.model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "   \n",
    "    def outputs(self):\n",
    "        return self.model.outputs\n",
    "    \n",
    "    def inputs(self):\n",
    "        return self.model.inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__ daughter class of NN_model, focus on functionality\n",
    "class NN_mnist(NN_model):\n",
    "    def __init__ (self, name, n_classes, X, Y, X_raw, Y_raw, X_val, Y_val, n_center = 10, initializer=InitCentersKmean, beta=10):\n",
    "        self.X_raw = X_raw  # input original data \n",
    "        self.Y_raw = Y_raw  # input original label         \n",
    "        self.X = X  # input  data after normalization for mean and variance \n",
    "        self.Y = Y  # input label after one_hot conversion\n",
    "        self.X_val = X_val # validation data during the model.fit\n",
    "        self.Y_val = Y_val # validation label during model.fit\n",
    "                \n",
    "        super().__init__(name, X, n_classes, n_center, beta, initializer) # initialize parents class\n",
    "    \n",
    "    # everything else is the same except the objects included in this function, in this case, self.X, self.X_raw, self.Y_raw\n",
    "    def __call__(self, X):\n",
    "        if(X.ndim != X_train.ndim):  # the input shape is (nsample, singe_image.shape). \n",
    "                                       # If a single image is the input, it need to expand to the \"nsample\" dimension\n",
    "            print(\"++single image: expanding dimentions to add nsample dim +++\")\n",
    "            X = np.expand_dims(X, axis=0)\n",
    "        self.X = X\n",
    "        \n",
    "        return self.get_logits(), self.get_probs()\n",
    "    \n",
    "    # plot the loss and accuracy of the training as a function of epoch\n",
    "    def plot_training_history(self):\n",
    "        fig = plt.figure()\n",
    "        plt.subplot(2,1,1)\n",
    "        plt.plot(self.history.history['acc'])\n",
    "        plt.plot(self.history.history['val_acc'])\n",
    "        plt.title('model accuracy')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='lower right')\n",
    "\n",
    "        plt.subplot(2,1,2)\n",
    "        plt.plot(self.history.history['loss'])\n",
    "        plt.plot(self.history.history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper right')\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "    \n",
    "    # plot images samples\n",
    "    def plot(self, indices, n_sample, figsize):\n",
    "        fig, axes = plt.subplots(3,n_sample,figsize=figsize, subplot_kw={'xticks':[], 'yticks':[]}) #subplot_kw option removes ticks in X, Y axes\n",
    "\n",
    "        for i in range(n_sample):\n",
    "            index=indices[i]\n",
    "            #  the original image before the normalization (this should be used for adversarial attacks)\n",
    "            axes[0, i].imshow(self.X_raw[index], cmap='gray', interpolation='none')\n",
    "            axes[0, i].set_title(\"True: {}  predict: {}\".format(self.Y_raw[index], self.predicted_classes[index]))\n",
    "\n",
    "            # normalized image. The background become uneven due to the normalization. Looks like already has ADATK\n",
    "            a = self.X[index].reshape(28, 28)\n",
    "            axes[1, i].imshow(a, cmap='gray', interpolation='none')\n",
    "\n",
    "            # 1-D histogram of normalized image\n",
    "            axes[2, i].hist(self.X[index], bins=20)\n",
    "            \n",
    "        fig.tight_layout()\n",
    "        fig.subplots_adjust(hspace=0.01)\n",
    "    \n",
    "\n",
    "    # plot image samples include original, normalized. Need to be updated to included ada image later.\n",
    "    def plot_random_sample(self, X, X_raw, Y_raw, which_to_plot, n_sample = 8, figsize=(15,5)):   \n",
    "        self.X = X\n",
    "        self.X_raw = X_raw\n",
    "        self.Y_raw = Y_raw\n",
    "        \n",
    "        # identify the indices of correctly and incorrectly recognized images\n",
    "        self.identify_correct_incorrect_prediction()\n",
    "        \n",
    "        if which_to_plot == \"correct\":\n",
    "            print(\"__correctly identified image _____\")\n",
    "            indices = np.random.choice(self.correct_indices, size=n_sample) # random indices of correctly identifed images \n",
    "        elif which_to_plot == \"incorrect\":\n",
    "            print(\"__wrongly identified image _____\")\n",
    "            indices = np.random.choice(self.incorrect_indices, size=n_sample) # random indices of incorrectly identified images in the data\n",
    "        else:\n",
    "            print(\"___ try again on inputs: correct or incorrect ___\")\n",
    "\n",
    "        self.plot(indices, n_sample, figsize)\n",
    "        \n",
    "    # evulated the model\n",
    "    def eval(self):\n",
    "        loss_and_metrics = self.model.evaluate(self.X, self.Y, verbose=2) # the metric is defined when compiling the model\n",
    "        print(\"__Loss:  \", loss_and_metrics[0])\n",
    "        print(\"__Accuracy: \", loss_and_metrics[1])\n",
    "        \n",
    "    # identify correct or incorrect prediction indices which is used for plotting later\n",
    "    def identify_correct_incorrect_prediction(self): \n",
    "        print(\"___ calculating the correct and incorrect prediction _____\")\n",
    "        self.predicted_classes = np.argmax(self.model.predict(self.X), axis=-1)\n",
    "        self.correct_indices = np.nonzero(self.predicted_classes == self.Y_raw)[0]\n",
    "        self.incorrect_indices = np.nonzero(self.predicted_classes != self.Y_raw)[0]\n",
    "        print(\"__complete___\")\n",
    "        \n",
    "            # run the model, save the weight and plot the training history\n",
    "    def run(self, batch_size=100, epochs=1, verbose=2):\n",
    "        self.history = self.model.fit(self.X, self.Y,\n",
    "                                      batch_size,\n",
    "                                      epochs,\n",
    "                                      verbose,\n",
    "                                      validation_data=(self.X_val, self.Y_val))\n",
    "        # saving the model\n",
    "        \"\"\"\n",
    "        # loading model has problem this way. it complain RBF layer is not unknow. \n",
    "        save_dir = \"./results/\"\n",
    "        model_name = 'RBF.h5'\n",
    "        model_path = os.path.join(save_dir, model_name)\n",
    "        model.save(model_path)\n",
    "        print('Saved trained model at %s ' % model_path)\n",
    "        \"\"\"\n",
    "        print(\"___ saving the model weight____\")    \n",
    "        self.model.save_weights(self.weight_file)\n",
    "                \n",
    "        # plot the history of the training \n",
    "        self.plot_training_history()\n",
    "        \n",
    "    # load the model weight. Currently the can not save the entire model since it compile RBFlayer is not defined\n",
    "    def load_model(self): \n",
    "        if os.path.exists(self.weight_file):\n",
    "            self.model.load_weights(self.weight_file)\n",
    "        else:\n",
    "            print(\"++++ no model weight file: \", self.weight_file, \"can be found ++++\")\n",
    "            exit()\n",
    "\n",
    "\n",
    "    # get output from a layers. In this case, for logits\n",
    "    def get_logits(self):\n",
    "\n",
    "        self.logits_layer_model = Model(inputs=self.model.input, outputs=self.model.get_layer('logits').output)\n",
    "  \n",
    "        return self.logits_layer_model.predict(self.X)\n",
    "    \n",
    "    # get the probability of each class\n",
    "    def get_probs(self):\n",
    "        return self.model.predict(self.X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wxie\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'n_center' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f3c2f9b6ee54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mX_train_raw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train_raw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_raw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test_raw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_mnist_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNN_mnist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_raw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train_raw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_center\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInitCentersKmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'n_center' is not defined"
     ]
    }
   ],
   "source": [
    "n_classes = 10\n",
    "n_center = 0\n",
    "beta = 0\n",
    "\n",
    "model_ID = 1\n",
    "\n",
    "if model_ID ==1:\n",
    "    model_name = \"dense_1L\"\n",
    "elif model_ID ==2:\n",
    "    model_name = \"dense\"\n",
    "elif model_ID==3:\n",
    "    n_center = 10\n",
    "    beta = 10\n",
    "    model_name = \"RBF_10center\"\n",
    "elif model_ID==4:\n",
    "    n_center = 50\n",
    "    beta = 10\n",
    "    model_name = \"RBF_50center\"\n",
    "elif mode_ID==5:\n",
    "    model_name = \"conv\"\n",
    "\n",
    "X_train_raw, Y_train_raw, X_test_raw, Y_test_raw, X_train, Y_train, X_test, Y_test = load_mnist_data(n_classes)\n",
    "\n",
    "model = NN_mnist(model_name, n_classes,  X_train, Y_train, X_train_raw, Y_train_raw, X_test, Y_test, n_center, InitCentersKmean, beta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100 \n",
    "epochs=20\n",
    "verbose=1\n",
    "is_training = False   # training or loading a model\n",
    "\n",
    "if is_training:\n",
    "    model.run(batch_size, epochs, verbose)\n",
    "else:\n",
    "    model.load_model()\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this is the training sample\n",
    "model.plot_random_sample(X_train, X_train_raw, Y_train_raw, \"incorrect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the test sample \n",
    "model.plot_random_sample(X_test, X_test_raw, Y_test_raw, \"correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the graph in binary .pb file. Convert keras model to a tensorflow model\n",
    "outdir = \"keras_to_TF\"\n",
    "try:\n",
    "    os.mkdir(outdir )\n",
    "except:\n",
    "    pass\n",
    "\n",
    "prefix = model_name \n",
    "name = prefix+'_output_graph.pb' # Alias the outputs in the model - this sometimes makes them easier to access in TF\n",
    "\n",
    "pred = []\n",
    "pred_node_names = []\n",
    "for i, o in enumerate(model.outputs()):\n",
    "        pred_node_names.append(prefix+'_'+str(i))\n",
    "        pred.append(tf.identity(o, name=pred_node_names[i]))\n",
    "print('Output nodes names are: ', pred_node_names[0])\n",
    "\n",
    "if is_training: # converting keras model to TF model\n",
    "    sess = K.get_session()\n",
    "    constant_graph = graph_util.convert_variables_to_constants(sess, \n",
    "                                                               sess.graph.as_graph_def(), \n",
    "                                                               pred_node_names)\n",
    "    graph_io.write_graph(constant_graph, outdir, name, as_text=False)\n",
    "    ## Finally delete the Keras's session\n",
    "    K.clear_session()\n",
    "    \n",
    "#load the tensorflow model from the file\n",
    "#tf.reset_default_graph()\n",
    "\n",
    "def load_graph(model_name):\n",
    "    graph = tf.get_default_graph()\n",
    "    graph_def = tf.GraphDef()\n",
    "    with open(model_name, \"rb\") as f:\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    with graph.as_default():\n",
    "        tf.import_graph_def(graph_def)\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print out the operation name for double check. No need to run if one is sure about the name of the operation\n",
    "for i, op in enumerate(tf.get_default_graph().get_operations()):\n",
    "    print (\"{: 3.0f}: {}\".format(i,op.name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the tensorflow model. All one needs are input and output operation and logits operation.  \n",
    "# load the model\n",
    "my_graph = load_graph(model_name=os.path.join(outdir, name))\n",
    "input_op = my_graph.get_operation_by_name(\"import/\"+model_name+\"_input\") # name of input operation \n",
    "output_op = my_graph.get_operation_by_name(\"import/\"+pred_node_names[0]) # name of output operation\n",
    "logit_op = my_graph.get_operation_by_name(\"import/logits/BiasAdd\") # name of logits operation. Double checked from keras model output\n",
    "\n",
    "ops = (input_op,output_op, logit_op)\n",
    "\n",
    "print(input_op.outputs[0].shape)\n",
    "print(output_op.outputs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate adversarial image  \n",
    "class AdversarialImage(object):\n",
    "    def __init__(self, inp, out, logits):\n",
    "        '''\n",
    "        inp : input tensor  (image)\n",
    "        out : output tensor (y_pred)\n",
    "        eps : scalar\n",
    "        '''\n",
    "        self.inp = inp.outputs[0] # input tensor\n",
    "        self.out = out.outputs[0] # output tensor\n",
    "        self.logits = logits.outputs[0] # logits tensor\n",
    "    \n",
    "    # the input shape is (nsample, singe_image.shape). \n",
    "    # If a single image is the input, it need to expand to the \"nsample\" dimension\n",
    "    def check_dim(self, image):\n",
    "        if(image.ndim != X_train.ndim):  \n",
    "            print(\"++single image: expanding dimentions to add nsample dim +++\")\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "        \n",
    "        return image\n",
    "\n",
    "    # obtain adversarial image\n",
    "    def get_adv_image(self, image, goal_class, nsteps, eps=1, beta = 0.01, verbose=0):\n",
    "        \"\"\"\n",
    "        image: original image \n",
    "        goal_class:  the goal class of the adversarial prediction from 0-9 \n",
    "        nsteps: number of steps to perform gradient descent\n",
    "        eps: learning rate \n",
    "        beta: learning rate for the 2nd term in the loss\n",
    "        verbose: 0: loss, 1: prob, logistic, loss\n",
    "        \n",
    "        \"\"\"\n",
    "        image = self.check_dim(image) # make sure the dim is (nsample, image)\n",
    "        org_image = image # for the 2nd term of the loss \n",
    "        \n",
    "        y_goal_tf = tf.expand_dims(tf.one_hot(goal_class, n_classes), axis=0) # the expansion take care of the nsample dimension\n",
    "        y_pred_tf = self.logits \n",
    "\n",
    "        #adv_image = self.inp - eps*grad_tf # adversarial image tensor going along the gradient direction    \n",
    "        org_image_tf = tf.placeholder(tf.float32, shape=image.shape)\n",
    "        \n",
    "        # this loss is a lot more effective than MSE loss function\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_pred_tf, labels=[y_goal_tf])\n",
    "        grad_tf  = tf.gradients(loss, self.inp)[0] # gradients of loss w.r.t. the image\n",
    "\n",
    "        #adv_image = tf.zeros(image.shape)\n",
    "        adv_image = self.inp\n",
    "        noise_image = tf.zeros(image.shape) # noise added on to the original image\n",
    "        \n",
    "        noise_per_step = eps*(grad_tf + beta*(adv_image - org_image_tf))\n",
    "        \n",
    "        adv_image -= noise_per_step   # adversarial image tensor going along the gradient direction        \n",
    "        noise_image += noise_per_step\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            # gradient descent\n",
    "            for i in range(nsteps):\n",
    "                # need to update image everytime\n",
    "                adv, noise, vloss, lg, grad, = sess.run([adv_image, noise_image, loss, y_pred_tf,  grad_tf ], \n",
    "                                                        feed_dict={self.inp:image, org_image_tf:org_image})\n",
    "                image = adv # update image every step. the updated image is fed to the self.inp\n",
    "                \n",
    "                if verbose != 0:\n",
    "                    print(\"loss: \", vloss)\n",
    "                    print(\"grad: \", grad[0][0:10])\n",
    "                    print(\"logitis:\", lg)\n",
    "\n",
    "                if(i%200 == 0):\n",
    "                    print(\"loss: \", vloss)\n",
    "                 \n",
    "        return adv, noise # numpy arrage of image\n",
    "    \n",
    "    # model prediction of the input image X. can be the original or the adv_image\n",
    "    def predict(self, image):\n",
    "        \"\"\"\n",
    "        image: input image\n",
    "        \n",
    "        \"\"\"\n",
    "        image = self.check_dim(image) # check to make the input image shape is (nsample, image)\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            probs, logits = sess.run([self.out, self.logits], feed_dict={self.inp:image})\n",
    "            \n",
    "        return probs, logits\n",
    "\n",
    "    # draw input image together with the probability of the image\n",
    "    def plot(self, input_images, correct_class, goal_class, figsize=(22, 3)):\n",
    "        \"\"\"\n",
    "        input_images[0]:--> orignial image\n",
    "        input_images[1]:--> adversarial image\n",
    "        input_images[2]:--> added noise, i.e. adv_image - org_image\n",
    "        correct_class:--> correct label\n",
    "        goal_class:--> goal label for adversarial \n",
    "    \n",
    "        \"\"\"\n",
    "        num_of_image = len(input_images)\n",
    "        probs = np.ndarray(shape=(num_of_image, 1, n_classes)) # define an array\n",
    "        for i in range(num_of_image):\n",
    "            input_images[i] = self.check_dim(input_images[i]) # check to make the input image shape is (nsample, image)\n",
    "            probs[i], _ = self.predict(input_images[i]) # probs[i].shape = (1, 10)      \n",
    "            \n",
    "        # subplots with nrow rows and ncoll columns\n",
    "        nrow = 1\n",
    "        ncoll = 6\n",
    "        fig, axes = plt.subplots(nrow, ncoll, figsize=figsize)\n",
    "        \n",
    "        ith = 0 # index of input_images[]\n",
    "        for i in range(0, ncoll, 2):  # remove ticks on axes of images\n",
    "            axes[i].set_xticks([])\n",
    "            axes[i].set_yticks([])\n",
    "            axes[i].imshow(input_images[ith].reshape(X_train_raw[0].shape), cmap='gray', interpolation='none')\n",
    "            ith += 1\n",
    "            \n",
    "        ith = 0 # index of probs[]\n",
    "        for i in range(1, ncoll, 2):  \n",
    "            barlist = axes[i].bar(range(n_classes), probs[ith][0]) # draw bar in the plot\n",
    "            ith +=1  \n",
    "\n",
    "            barlist[goal_class].set_color('r')\n",
    "            barlist[correct_class].set_color('g')\n",
    "            \n",
    "            axes[i].set_ylim([0, 1.1])\n",
    "            axes[i].set_ylabel('Probility')\n",
    "            axes[i].set_xticks(range(n_classes))\n",
    "            axes[i].set_xticklabels(str(i) for i in range(n_classes))\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv = AdversarialImage(*ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-85948b681fc2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mcorrect_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY_test_raw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mith\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0madv_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_adv_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morg_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgoal_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'adv' is not defined"
     ]
    }
   ],
   "source": [
    "ith = 100  # ith sample in the data\n",
    "goal_class = 1\n",
    "nstep = 2000\n",
    "epsilon = 1\n",
    "beta = 0.001\n",
    "\n",
    "org_image = X_test[ith]\n",
    "correct_class = Y_test_raw[ith]\n",
    "\n",
    "adv_image, noise_image = adv.get_adv_image(org_image, goal_class, nstep, epsilon, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adv_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-f16653cefaed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0minput_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0morg_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madv_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise_image\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0madv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorrect_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgoal_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'adv_image' is not defined"
     ]
    }
   ],
   "source": [
    "input_images = [org_image, adv_image, noise_image]\n",
    "\n",
    "adv.plot(input_images, correct_class, goal_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
