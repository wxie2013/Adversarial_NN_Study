{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is based on: \n",
    "    https://github.com/PetraVidnerova/rbf_keras\n",
    "    https://keras.io/layers/writing-your-own-keras-layers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "import gzip\n",
    "import _pickle as cPickle\n",
    "\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.initializers import RandomUniform, Initializer, Orthogonal, Constant\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import math\n",
    "import random \n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for array-handling and plotting\n",
    "from keras.datasets import mnist\n",
    "\n",
    "n_classes = 10\n",
    "def load_data():\n",
    "\n",
    "    (X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "        \n",
    "    # Normalizing the input data to max=1 helps to speed up the training. \n",
    "    # reshape each image to 1D vector for simplicity\n",
    "    n_train = X_train.shape[0]\n",
    "    n_test = X_test.shape[0]\n",
    "    v_len = X_train.shape[1]*X_train.shape[2]    \n",
    "    \n",
    "    X_train = X_train.reshape(n_train, v_len)\n",
    "    X_test = X_test.reshape(n_test, v_len)\n",
    "    \n",
    "    # normalizing the data to help with the training. accurancy is lower than sklearn.preprocessing.scale\n",
    "    \"\"\"\n",
    "    max_pixel = np.max(X_train)\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    X_train /= max_pixel\n",
    "    X_test /= max_pixel\n",
    "    \"\"\"\n",
    "    #using sklearn.preprocessing.scale, leads to higher accuracy than normalization by max as above\n",
    "    X_train = scale(X_train)\n",
    "    X_test = scale(X_test)\n",
    "\n",
    "    \n",
    "    #Do it only when the final layers output is, e.g. 10\n",
    "    # one-hot encoding using keras' numpy-related utilities\n",
    "    print(\"Shape before one-hot encoding: \", Y_train.shape)\n",
    "    Y_train = np_utils.to_categorical(Y_train, n_classes)\n",
    "    Y_test = np_utils.to_categorical(Y_test, n_classes)\n",
    "    print(\"Shape after one-hot encoding: \", Y_train.shape)\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data sets\n",
    "X_train, Y_train, X_test, Y_test = load_data() \n",
    "\n",
    "#check if still produce images\n",
    "fig, axes = plt.subplots(2,10,figsize=(15, 3))\n",
    "for i in range(20):\n",
    "    if(i<10):\n",
    "        a = X_train[i].reshape(28, 28)\n",
    "        axes[0, i].imshow(a, cmap='gray', interpolation='none')\n",
    "        axes[0, i].set_title(\"train: {}\".format(Y_train[i]))\n",
    "\n",
    "    else: \n",
    "        a = X_test[i-10].reshape(28, 28)\n",
    "        axes[1, i-10].imshow(a, cmap='gray', interpolation='none')\n",
    "        axes[0, i-10].set_title(\"test: {}\".format(Y_test[i]))\n",
    "\n",
    "#check the 1-D distribution of each image        \n",
    "fig, axes = plt.subplots(2,10,figsize=(15, 3))\n",
    "for i in range(20):\n",
    "    if(i<10):\n",
    "        axes[0, i].hist(X_train[i], bins=20)\n",
    "        axes[0, i].set_title(\"train: {}\".format(Y_train[i]))\n",
    "\n",
    "    else: \n",
    "        axes[1, i-10].hist(X_test[i], bins=20)\n",
    "        axes[0, i-10].set_title(\"test: {}\".format(Y_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializer for RBF\n",
    "class InitCentersKmean(Initializer):\n",
    "    \"\"\" Initializer for initialization of centers of RBF network\n",
    "        using K-mean.\n",
    "\n",
    "    # Arguments\n",
    "        X: matrix, dataset to choose the centers from (random rows \n",
    "          are taken as centers)\n",
    "    \"\"\"\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "        print(\"_____ initializing center using InitCentersKmean _____\")\n",
    "\n",
    "    def __call__(self, shape, dtype=None):\n",
    "        # \"shape\" is the input_shape for \"self.add_weigh\" down below\n",
    "        self.center = sklearn.cluster.k_means(self.X, init='random', n_clusters=shape[0], max_iter = 1, n_jobs=-1)\n",
    "        \n",
    "        return self.center[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "#the custom RBF layer        \n",
    "class RBFLayer(Layer):\n",
    "    \"\"\" Layer of Gaussian RBF units. \n",
    "\n",
    "    # Example\n",
    " \n",
    "    ```python\n",
    "        model = Sequential()\n",
    "        model.add(RBFLayer(10,\n",
    "                           initializer=InitCentersRandom(X), \n",
    "                           betas=1.0,\n",
    "                           input_shape=(1,)))\n",
    "        model.add(Dense(1))\n",
    "    ```\n",
    "    \n",
    "\n",
    "    # Arguments\n",
    "        output_dim: number of hidden units (i.e. number of outputs of the layer)\n",
    "        initializer: instance of initiliazer to initialize centers\n",
    "        betas: float, initial value for betas \n",
    "\n",
    "    \"\"\"\n",
    "   \n",
    "    def __init__(self, output_dim, initializer=InitCentersKmean, betas=1.0, **kwargs):\n",
    "        self.output_dim = output_dim # dim of the layer's output, i.e. in this case: number of centers in this layer\n",
    "        self.init_betas = betas \n",
    "        self.initializer = initializer \n",
    "\n",
    "        super().__init__(**kwargs) #python3 format\n",
    "\n",
    "    def build(self, input_shape): # input shape is the shape from output of last layers. in this case, it's data: [?, 784]\n",
    "        self.centers = self.add_weight(name='centers', \n",
    "                                       shape=(self.output_dim, input_shape[1]),\n",
    "                                       initializer=self.initializer,\n",
    "                                       trainable=True)\n",
    "        \n",
    "        self.betas = self.add_weight(name='betas',\n",
    "                                     shape=(self.output_dim,), \n",
    "                                     initializer=Constant(value=self.init_betas),\n",
    "                                     trainable=True)\n",
    "        \n",
    "        super().build(input_shape) #python3\n",
    "     \n",
    "    def call(self, x):\n",
    "\n",
    "        C=K.expand_dims(self.centers) # shape (ncenter, 784, 1) for broadcast\n",
    "        H = K.transpose(C-K.transpose(x)) # K.transpose(x) shape (784, ?), then this shape: \n",
    "\n",
    "        \"\"\"\n",
    "        # the following printout is used for debugging purpose\n",
    "        print(\"self.centers.shape: \", self.centers.shape)\n",
    "        print(\"x.shape\", x.shape)\n",
    "        print(\"K.transpose(x).shape: \", K.transpose(x).shape)\n",
    "        print(\"C.shape: \", C.shape)\n",
    "        print(\"H.shape: \", H.shape)\n",
    "        print(\"self.betas.shape: \", self.betas.shape)\n",
    "        print(\"K.sum(H**2, axis=-2).shape: \", K.sum(H**2, axis=-2).shape)\n",
    "        print(\"call return shape: \", K.exp( -self.betas * K.sum(H**2, axis=-2)).shape)\n",
    "        \"\"\"\n",
    "        a = K.exp( -K.sum(H**2, axis=-2)/(2*self.betas*self.betas))\n",
    "        \n",
    "        return a\n",
    "             \n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        # have to define get_config to be able to use model_from_json\n",
    "        config = {\n",
    "            'output_dim': self.output_dim\n",
    "        }\n",
    "        #base_config = super(RBFLayer, self).get_config()\n",
    "        base_config = super().get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now build the model and run the training \n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "n_center = 10\n",
    "one_sample_shape = X_train.shape[-1] # shape for each individual sample\n",
    "\n",
    "rbflayer = RBFLayer(n_center,\n",
    "                    initializer=InitCentersKmean(X_train), \n",
    "                    betas=20,\n",
    "                    # input_shape is the data shape. The 2nd dim of (one_sample_shape, ) is the number of samples and is treated as ? in keras\n",
    "                    input_shape=(one_sample_shape, )) \n",
    "\n",
    "model.add(rbflayer)\n",
    "model.add(Dense(n_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(loss='mean_squared_error',             optimizer=RMSprop())\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "          batch_size=100,\n",
    "          epochs=20,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_train)\n",
    "print(Y_train[4])\n",
    "print(Y_pred[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outputs(inputs, model, Lin, Lout):\n",
    "    layer_fn = K.function([model.layers[Lin].input], \n",
    "                             [model.layers[Lout].output]) \n",
    "    \n",
    "    return  layer_fn([inputs,1])[0]\n",
    "\n",
    "out_1 = get_outputs(X_train[0:1], model, 0, 0)\n",
    "print(X_train[0:1].shape)\n",
    "print(out_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "\"\"\"\n",
    "# loading model has problem this way. \n",
    "save_dir = \"./results/\"\n",
    "model_name = 'RBF.h5'\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\"\"\"\n",
    "model.save_weights('results/RBF_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the metrics\n",
    "fig = plt.figure()\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='lower right')\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
